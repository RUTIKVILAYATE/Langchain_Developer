{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Chatbot\n",
    "\n",
    "# How to design and implement an LLM based chatbot\n",
    "# This chatbot will be able to have conversations and remember previous interactions\n",
    "\n",
    "# chatbot will only use language model to have a conversation\n",
    "# while other related concepts can be integrated:\n",
    "# RAG -> Conversational RAG -> Enable a chatbot experience over an external source of data\n",
    "# Agents -> Build a chatbot that can take actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "form dotenv import load_dotenv\n",
    "load_dotenv() # loading all the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access specific groq model \n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoking model using HumanMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My Name is Anonymous and I am a chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make converation within a list it will remember all the conversation\n",
    "from langchain_core.messages import AIMessage\n",
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi my name is Anonymous and I am a chief AI Engineer\"),\n",
    "    AIMessage(content=\"Hello, Anonymous! It's nice to meet you!, As a Cheif Engineer what kind of project are you working on currently \"),\n",
    "    HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MessageHistory \n",
    "# We can use a Message History class to wrap out model and make it stateful. \n",
    "# This will keep track of input and output of the model and store them in some datastore\n",
    "# Future interactions will then load those messages and pass them into the chain as part of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Individual conversation with Individuals\n",
    "# means distinguishing one chat history with the other \n",
    "# history with respect to session id will be created and this session_id will be used to distinguish one session id with the other\n",
    "\n",
    "\n",
    "# check session_history is available or not if not it will create session history and store it in store{}\n",
    "# otherwise it will return the previous chat history which was remembered earlier \n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Interact with llm model based on the chat history\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when given config -> that means for the specific id we are interacting\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Anonymous and I am an AI Chief Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(config=\"What is my name?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the config -> session id \n",
    "# if passed chat1 it will remember the previous name given \n",
    "# if passed chat2 it will not remember the previous name and will say that he dont have memory of past converrsations\n",
    "config1 = {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check by edition the new message\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey! My name is John Cena\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if remember or not by providing same config\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name\")],\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Prompt Template and MessageChat History using Langchain \n",
    "# Prompt Template -> Helps turn raw user information into a format that LLM can work with\n",
    "# since raw user input is just a message -> which we are passing to the LLM \n",
    "# to make it complicated \n",
    "# add system message with some custom instruction (but still taking messages as input)\n",
    "# add in more input besides just messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \"You are a helpful assistant. Answer all the quuestion to the nest of your ability\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "\n",
    "]\n",
    ")\n",
    "\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"messages\": [HumanMessage(content=\"Hi my name is Anonymous \")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\": \"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Anonymous\")],\n",
    "    config= config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all the question to the best of your ability in this {language}\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Anonymous\")], \"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap this more complicated chain in a message history class\n",
    "# this time because there are multiple keys in the input we need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\":\"chat4\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi, I am Anonymous\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Manage the Chat Conversation History \n",
    "# How to manage conversation history\n",
    "# bcoz if left unmanaged, list of messages will grow unbounded and potentially overflow the context window of the llm\n",
    "# therefore it is important to add a step that limits the size of the messages you are passing in \"trim_messages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 70,                    # allow maximum tokens -> 70 \n",
    "    strategy = \"last\",                  # count the tokens from last session \n",
    "    token_counter = model,              # token will be counted by model\n",
    "    include_system = True,              # whether or not include message by system \n",
    "    allow_partial = False,             # other content\n",
    "    start_on = \"human\"                  # count human tokens only\n",
    "\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm Bob!\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no Problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model \n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"What ice cream do I like\")],\n",
    "    \"language\": \"English\"\n",
    "}\n",
    ")\n",
    "response.content\n",
    "\n",
    "# if not got answer -> maybe it go trimmed bcoz trimmer is passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "    \"language\": \"English\",\n",
    "}\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke( \n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "    \"language\": \"English\",\n",
    "},\n",
    "config = config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke( \n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"what math problem did I ask?\")],\n",
    "    \"language\": \"English\",\n",
    "},\n",
    "config = config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector stores and Retrievers \n",
    "# Familiarize with Langchain's vector and retriever abstractions\n",
    "# Abstractions are designed to support retrieval of data -> from vector databases and other sources for integrations with LLM workflows\n",
    "# They are imporatant for applications that fetched data to be reasoned as part of the model inference\n",
    "# as in the case of retrieval augmented generation\n",
    "\n",
    "\n",
    "# Document\n",
    "# Vector Stores\n",
    "# Retrievers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents ->\n",
    "# Langchain implements a document abstraction, which is intended to represent a unit of text and associated metadata\n",
    "# It has two attributes: \n",
    "# page_content: a string representing the content\n",
    "# metadata: a dictionary containing arbitrary metadata.\n",
    "# The metadata attribute can capture information about the source of the document, its relationship to other documents\n",
    "# and other information\n",
    "# note that -> an individual document object often represent a chunk of a larger document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to create document \n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model=\"Llama3-8b-8192\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorStores\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore=Chroma.from_documents(documents,embedding=embeddings)\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Async query\n",
    "await vectorstore.asimilarity_search(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search_with_score(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrievers\n",
    "# LangChain VectorStore objects do not subclass Runnable, and so cannot immediately be integrated into LangChain Expression Language chains.\n",
    "\n",
    "# LangChain Retrievers are Runnables, so they implement a standard set of methods \n",
    "# (e.g., synchronous and asynchronous invoke and batch operations) and are designed to be incorporated in LCEL chains.\n",
    "\n",
    "# We can create a simple version of this ourselves, without subclassing Retriever. \n",
    "# If we choose what method we wish to use to retrieve documents, we can create a runnable easily. \n",
    "# Below we will build one around the similarity_search method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever=RunnableLambda(vectorstore.similarity_search).bind(k=1)    # top result \n",
    "retriever.batch([\"cat\",\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. \n",
    "# These retrievers include specific search_type and search_kwargs attributes that identify what methods of \n",
    "# the underlying vector store to call, and how to parameterize them. \n",
    "# For instance, we can replicate the above with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "retriever.batch([\"cat\",\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "rag_chain={\"context\":retriever,\"question\":RunnablePassthrough()}|prompt|llm\n",
    "\n",
    "response=rag_chain.invoke(\"tell me about dogs\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
